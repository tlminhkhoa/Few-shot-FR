# -*- coding: utf-8 -*-
"""SVM FewShot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YfrRp-bgCgFJB2I8zSNN_mZCpKB4LO2k
"""

# pip install mtcnn

from google.colab import drive
drive.mount('/content/drive')

from keras.models import load_model
import mtcnn
from PIL import Image 
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from mtcnn.mtcnn import MTCNN
from sklearn.pipeline import Pipeline
import pandas as pd
import os
from numpy import savetxt,loadtxt
from keras.models import load_model
from sklearn.neighbors import KDTree
import time

# Loads the train + test datasets from the specified embedding CSVs, then splits the test set into eval and test sets based on the specified percentage (for test set)
# Returns the X,y arrays for all 3 (trainX, trainY, evalX, evalY, testX, testY)
# If an "other" set is specified, it is added to the eval dataset
# All zero-rows in the data (empty face embeddings) are silently removed
def loadAndSplitDatasets(trainEmbeddingsCSVPath, testEmbeddingsCSVPath, percentEval, otherEmbeddingsCSVPath = ""):
    if percentEval > 1:
        percentEval = percentEval/100 # Interpret any number > 1 as being /100
    if percentEval == 1:
        raise Exception("You can't split the train set into train/eval with a ratio of 100%!")
    trainAndEvalXY = datasetDataFramesToNumpyArrays(load_train_embeddings_from_per_identity_CSV_directory(trainEmbeddingsCSVPath), False)
    (testX, testY) = datasetDataFramesToNumpyArrays(load_train_embeddings_from_per_identity_CSV_directory(testEmbeddingsCSVPath))
    
    (rows, cols) = trainAndEvalXY.shape
    trainAndEvalXYListByLabel = [trainAndEvalXY[trainAndEvalXY[:, cols-1] == k] for k in np.unique(trainAndEvalXY[:, cols-1])]
    trainXY = []
    evalXY = []
    for array in trainAndEvalXYListByLabel:
        size = len(array[:, -1])
        trainEndIndex = int(round((size * (1 - percentEval)) - 1))
        (trainXYthis, evalXYthis) = (array[:trainEndIndex,:], array[trainEndIndex+1:,:])
        trainXY.append(trainXYthis)
        evalXY.append(evalXYthis)
    trainXY = np.vstack(trainXY)
    evalXY = np.vstack(evalXY)
    (trainX, trainY) = (trainXY[:, :-1], trainXY[:, -1])
    (evalX, evalY) = (evalXY[:, :-1], evalXY[:, -1])
    if (len(otherEmbeddingsCSVPath) > 0):
        (otherX, otherY) = datasetDataFramesToNumpyArrays(load_train_embeddings_from_per_identity_CSV_directory(otherEmbeddingsCSVPath))
        evalX = np.concatenate((evalX, otherX), axis=0)
        evalY = np.concatenate((evalY, otherY))
    else:
        otherY = []
    print("Loaded data.       Sizes: " + str(len(trainY)) + " train, " + str(len(evalY)) + " eval (of which 'other' is " + str(len(otherY)) + "), " + str(len(testY)) + " test")
    (trainX, trainY) = dropZeroRows(trainX, trainY)
    (testX, testY) = dropZeroRows(testX, testY)
    (evalX, evalY) = dropZeroRows(evalX, evalY)
    print("Dropped zero rows. Sizes: " + str(len(trainY)) + " train, " + str(len(evalY)) + " eval, " + str(len(testY)) + " test")
    return (trainX, trainY, testX, testY, evalX, evalY)

# Load dataset embeddings from all the CSV files in the given directory (the CSV should have been generated by the datasetToEmbeddingCSVs function above)
def load_train_embeddings_from_per_identity_CSV_directory(directory):
    df = pd.DataFrame()
    labels = []
    for filename in os.listdir(directory):
        if filename.endswith(".csv"):
            lines = loadtxt(os.path.join(directory,filename), delimiter=',', skiprows=1, usecols=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127)).reshape(-1,128)
            for row in lines:
                row = row.reshape(1,128)
                if(df.empty):
                    df = pd.DataFrame(row)
                else:
                    df = pd.concat([df,pd.DataFrame(row)])
                labels.append(filename.replace(".csv",""))
    df["labels"] = labels
    return df

# Changing pandas DataFrame table to numpy arrays for easier processing (expects last column to be labels, rest to be features)
def datasetDataFramesToNumpyArrays(dataFrame, splitIntoXY = True):
    temp = dataFrame.to_numpy()
    if splitIntoXY is True:
        return (temp[:, :-1], temp[:, -1])
    else:
        return temp

# Given a numpy array of face embeddings X and corresponding labels Y, get rid of all samples which are all-zero, and their corresponding labels, and return the result
def dropZeroRows(datasetX, datasetY):
    resultX = []
    resultY = []
    for i, row in enumerate(datasetX):
        if np.sum(row) != 0:
            resultX.append(row)
            resultY.append(datasetY[i])
    return (np.array(resultX), np.array(resultY))

# loading up trainset ,testset
(trainX, trainY, testX, testY, evalX, evalY) = loadAndSplitDatasets("/content/drive/MyDrive/CPS803/img_celeba/train embedding", "/content/drive/MyDrive/CPS803/img_celeba/test embedding", 0.3, "")

# ---------------------------------------------------------------------------------------------------------------------------------------------------

# # using grid search to find the best parameter for the problem
# from sklearn import svm
# from sklearn import metrics


# from sklearn.model_selection import GridSearchCV
 
# # defining parameter range
# param_grid = {'C': [0.1, 1, 10, 100, 1000],
#               'gamma': [ "auto",1, 0.1, 0.01, 0.001, 0.0001],
#               'kernel': ["linear", "poly", "rbf", "sigmoid"]}
 
# grid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3,scoring='accuracy')
 
# # fitting the model for grid search
# grid.fit(trainX, trainY)

# # the following is the best parameter
# grid.best_params_

# fit the best parameter 
from sklearn import svm
from sklearn import metrics
import time
clf = svm.SVC(C= 10,gamma = 0.01 , kernel = "rbf",probability=True)

timeTakenTrainA = 0
tic = time.perf_counter()
clf.fit(trainX, trainY)
timeTakenTrainA = timeTakenTrainA + (time.perf_counter() - tic)

timeTakenTrainA

# Measure the train accuracy 
y_pred = clf.predict(trainX)
print("Accuracy:",metrics.accuracy_score(trainY, y_pred))

# Measure the evaluate accuracy 
print("Accuracy:",metrics.accuracy_score(evalY, clf.predict(evalX)))

# ----------------------------------------------------------------------------------------------------------

# get the probability distribution of the evaluate set
probaEval = clf.predict_proba(evalX)

# measure the entropy of the evaluate set's probability 
from scipy.stats import entropy
evalEntropy =  entropy(probaEval.T)

# Loading up the Other dataset
directory = "/content/drive/MyDrive/CPS803/img_celeba/Other embedding"
OtherEmbeddings = pd.DataFrame()
for filename in os.listdir(directory):
  df_temp = pd.read_csv(directory + "/"+filename)
  OtherEmbeddings = OtherEmbeddings.append(df_temp)

# get the probability distribution of the Other set
probaOther = clf.predict_proba(OtherEmbeddings.iloc[:,:-2].to_numpy())

# measure the entropy of the Other set's probability 
otherEntropy =  entropy(probaOther.T)

# plot to understand the 2 entropy distribution
from matplotlib import pyplot
bins = np.linspace(4, 6, 100)
from random import sample
evalEntropySample = sample(list(evalEntropy),len(otherEntropy))
pyplot.hist(evalEntropySample, bins, alpha=0.5, label='evalEntropy')
pyplot.hist(otherEntropy, bins, alpha=0.5, label='otherEntropy')
pyplot.legend(loc='upper right')
pyplot.show()

# -------------------------------------------------------------------------------------------------------------------------------

# create a new dataset to train the new model
df  = pd.DataFrame(probaEval)
df["label"] = "Trained"
df_temp  = pd.DataFrame(probaOther)
df_temp["label"] = "Other"
df = df.append(df_temp)
df

# train test split
from sklearn.model_selection import train_test_split
X_train_prob, X_test_prob, y_train_prob, y_test_prob = train_test_split(df.iloc[:,:-1], df.iloc[:,-1], test_size=0.33, random_state=42)

# use grid serach to find the best parameter 
from sklearn import svm
from sklearn import metrics


from sklearn.model_selection import GridSearchCV
 
# defining parameter range
param_grid = {'C': [0.1, 1, 10, 100, 1000],
              'gamma': [ "auto",1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ["linear", "poly", "rbf", "sigmoid"]}
 
grid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3,scoring='accuracy')
 
# fitting the model for grid search
grid.fit(X_train_prob, y_train_prob)

# the best parameter
grid.best_params_

clf2 = svm.SVC(C= 1000,gamma = 1 , kernel = "rbf",probability=True)
timeTakenTrainB = 0
tic = time.perf_counter()
clf2.fit(X_train_prob, y_train_prob)
timeTakenTrainB = timeTakenTrainB + (time.perf_counter() - tic)

timeTakenTrainB

# # roc curve for logistic regression model with optimal threshold
from numpy import sqrt
from numpy import argmax

from sklearn.metrics import roc_curve
an_array = y_test_prob
an_array = np.where(an_array == "Other", 0, an_array)
an_array = np.where(an_array == "Trained", 1, an_array)
an_array = an_array.astype(int)
yhat = clf2.predict_proba(X_test_prob)
# keep probabilities for the positive outcome only
yhat = yhat[:, 1]
# calculate roc curves
fpr, tpr, thresholds = roc_curve(an_array, yhat)
# calculate the g-mean for each threshold
gmeans = sqrt(tpr * (1-fpr))
# locate the index of the largest g-mean
ix = argmax(gmeans)
bestThresholds = thresholds[ix]
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
# plot the roc curve for the model
pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')
pyplot.plot(fpr, tpr, marker='.', label='SVM')
pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
# show the plot
pyplot.show()

def predict_clf2(X_test_prob,bestThresholds):
  returnList = []
  predictProba = clf2.predict_proba(X_test_prob)
  for i in range(predictProba.shape[0]):
    maxValue = np.max(predictProba[i])
    if maxValue > bestThresholds:
      returnList.append(argmax(predictProba[i]))
    else:
      returnList.append(np.argmin(predictProba[i]))
  new_items = ["Other" if x == 0 else "Trained" for x in returnList]
  return new_items

returnList = predict_clf2(X_test_prob,bestThresholds)
count =0
for i in range(len(returnList)):
  if returnList[i] == y_test_prob.tolist()[i]:
    count += 1

count/len(returnList)

print("Accuracy:",metrics.accuracy_score(y_train_prob, clf2.predict(X_train_prob)))

print("Accuracy:",metrics.accuracy_score(y_test_prob, clf2.predict(X_test_prob)))

# /------------------------------------------------------------------------------------------------------

# loading up the test set
directory = "/content/drive/MyDrive/CPS803/img_celeba/test embedding"
testEmbedding = pd.DataFrame()
for filename in os.listdir(directory):
  df_temp = pd.read_csv(directory + "/"+filename)
  testEmbedding = testEmbedding.append(df_temp)

def dropZeros(row):
  first = row.iloc[0]
  sumRow = sum(row.iloc[:-2])

  if (sumRow == 0 and first == 0):
    return None
  else:
    return row

testEmbedding = testEmbedding.apply(dropZeros,axis = 1).dropna()
testEmbedding

# re-label the test set to find the Other embdedding , person that is not in the train set
trainDict = "/content/drive/MyDrive/CPS803/img_celeba/subset train"
trainLabels = os.listdir(trainDict)
trainedEmbdedding = testEmbedding[testEmbedding["label"].isin(trainLabels)]
OtherEmbdedding = testEmbedding[ ~ testEmbedding["label"].isin(trainLabels)]
OtherEmbdedding["label"] = "Other"
df_test = trainedEmbdedding.append(OtherEmbdedding)
# shuffle the testset
df_test = df_test.sample(frac=1)
df_test = df_test.drop(["file"],axis = 1)
df_test

def measure_test_set_accuracy(df_test):
  # use the second model to classify the Other label
  X_test = df_test.iloc[:,:-1] 
  model2Prediction = clf2.predict(clf.predict_proba(X_test))
  # model2Prediction = np.array(predict_clf2(clf.predict_proba(X_test),bestThresholds))
  # count the correct answer from the second model
  correctOther = sum(df_test[model2Prediction == "Other"]["label"] == "Other")
  # cut out the rest of testset
  X_test_trained, Y_test_trained = (df_test[model2Prediction != "Other"].iloc[:,:-1], df_test[model2Prediction != "Other"]["label"])
  # use the first model to predict the rest of the testset
  test_predicted = clf.predict(X_test_trained)
  correct  = 0 
  for i in range(len(test_predicted)):
    if str(test_predicted[i]) == str(Y_test_trained.iloc[i]).replace(".0",""):
      correct += 1
  return (correct + correctOther)/df_test.shape[0]

print("The accuracy on the test set is",measure_test_set_accuracy(df_test))

import time
timeTaken = 0
tic = time.perf_counter()
print("The accuracy on the test set is",measure_test_set_accuracy(df_test))
timeTaken = timeTaken + (time.perf_counter() - tic)

timeTaken

labelEval = np.vstack((evalY.reshape(-1,1), OtherEmbeddings.iloc[:,-2].values.reshape(-1,1)))
trainEval = np.vstack((evalX, OtherEmbeddings.iloc[:,:-2]))
df_eval = pd.DataFrame(trainEval)
df_eval["label"] = labelEval
df_eval

# re-label the test set to find the Other embdedding , person that is not in the train set
trainDict = "/content/drive/MyDrive/CPS803/img_celeba/subset train"
trainLabels = os.listdir(trainDict)
trainedEmbdedding = df_eval[df_eval["label"].isin(trainLabels)]
OtherEmbdedding = df_eval[ ~ df_eval["label"].isin(trainLabels)]
OtherEmbdedding["label"] = "Other"
df_eval = trainedEmbdedding.append(OtherEmbdedding)
# shuffle the testset
df_eval = df_eval.sample(frac=1)
df_eval

import time
timeTaken = 0
tic = time.perf_counter()
print("The accuracy on the eval set is",measure_test_set_accuracy(df_eval))
timeTaken = timeTaken + (time.perf_counter() - tic)
print("Time Taken: ",timeTaken)

